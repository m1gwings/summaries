---
marp: true
theme: summary
math: mathjax
---
# Storage

<div class="author">

Cristiano Migali
(_adapted from the slides of Prof. Manuel Roveri_)

</div>

<style>
section {
    font-size: x-large;
}

.definition {
    padding-left: 0.5cm;
    padding-right: 0.5cm;
    background: var(--algorithms);
    border-radius: 0.5cm;
    border-style: solid;
    border-color: var(--text);
    border-width: 3pt;
    text-align: justify;
}
</style>

## Trends

The growth of the "_Global Datasphere_" (the volume of all the data created in the world) in the last 15 years has been _exponential_. Between 80s and 90s most of the data was generated by humans. Nowadays machines generate data at an unprecedented rate.
Along with this growth, we can observe a shift to centralized storage (storing data in the cloud) because of the ease of maintenance for the consumer.
Storage technology is (_still_) **dominated by HDDs**, even though the **fraction** of data **stored** in **SSD** devices is **growing**. A (_almost_) constant fraction of data is (**_still_**) stored in **tapes**.

There are also some hybrid HDD+SSD solutions:
- some large storage servers use _SSD_ as a cache for several _HDDs_;
- some HDD manufacturers produce **Solid State Hybrid Disks** (**SSHDs**) that combine a _small SSD_ with a _large HDD_ is a single unit.

---

## Disk abstraction

_Disks_ are seen by the OS as a **collection of data blocks** that can be _read or written independently_. To allow the ordering/management between them, each block is characterized by a unique numerical address called **Logical Block Address** (**LBA**).
Typically the OS _groups blocks_ into **clusters** to simplify the access to the disk. They are the _minimal unit_ the OS can read from ro write to a disk. Clusters' size ranges from 1 _sector_ (_see later_) (512 B) to 128 sectors (64 KiB).

### What's inside clusters?

We can distinguish between _4 types of clusters_ according to their _content_ (_and position_).
- Fixed position, meta-data clusters: are needed to bootstrap the file system (_locate the variable position meta data clusters_).
- Variable position, meta-data clusters: hold the folder structure.
- File data clusters: store the actual files' content.
- Unused space: they are clusters which can be used to store new files and folders.

In particular **meta-data clusters contain**:
- file names;
- directory structures and symbolic links;

---

- file size and file type;
- creation, modification, and last access dates of files;
- security information (owner, access lists, ...);
- **the LBA where the file content can be located on the disk**.

### Operating on disks
#### Reading

**Reading a file** requires to:
1. Access the meta-data to locate its blocks.
2. Access the blocks to read its content.

#### Writing

**Writing a file** requires to:
1. Access the meta-data to locate free space.
2. Write the data in the assigned block.

#### Deleting

**Deleting a file** requires to:
1. (_Just_) Update the meta-data to mark the blocks where the file was stored as free. 

---

**Remark**: deleting a file never actually deletes the data on the disk. When a new file will be written on the same clusters, the old data will be replaced by the new one.

### Disks fragmentation

#### Internal fragmentation

Since the file system can only access clusters, the _real occupation_ of space _by a file_ on the disk is always a _multiple of the cluster size_.
In particular: let $s$ be the file size, $c$ be the cluster size, $a$ be the actual size of the file on disk. Then:
$$
a = c \left\lceil \frac{s}{c} \right\rceil .
$$
The quantity $w = a - s$ is **wasted disk space** due to the organization of the file into clusters. This waste of space is called **internal fragmentation** of files.

#### External fragmentation

As the life of the disk evolves there might be not enough space to store a file in contiguous clusters. In this case the file is split into smaller chunks that are stored in the free clusters spread over the disk.
The effect of splitting a file into non-contiguous clusters is called **external fragmentation**.

---

## Devices
### Hard Disk Drive (HDD)

An **HDD** is a data storage device using rotated disks coated with magnetic material. Data is read and written in a _random-access_ manner, meaning that individual blocks of data can be stored and retrieved in any order.

#### Anatomy of a HDD

The rotating disks on which the HDD stores the data are known as **platters**: they are of equal radius, concentric, and vertically stacked on an axis known as **spindle**. Each platter is split in a fixed number of circular crowns, all with the same difference between external and internal radii, known as **tracks**. The set of tracks on different platters which have the same internal (and hence external) radius (i.e. they are one on top of the other) form a **cylinder**.
Each platter is split in a fixed number of circular sectors, all defined by the same angle; the intersection between a circular sector and a track on the same platter define a **sector**. Sectors are the _minimal units_ that can be written or read from an HDD. They usually allow to store 512 B or 4096 B. A _write to a single sector_ is _atomic_. _Multiple sectors writes_ instead can be interrupted (failing), we call these **torn writes**.

---

For each platter there is a **read-write head** which allows the storage and retrieval of data from sectors; the heads are mounted on an actuated arm which allows to place them on top of the desired track.
Finally, also the spindle is actuated to make the platters spin at a constant rate, measured in Revolutions Per Minute (RPM).

#### Caching for a HDD

Many disks incorporate caches, known as **track buffers**. Usually these are implemented through small amounts of RAM (8, 16, or 32 MiB). Flash memories are used instead to have persistent caching.

When _reading_, caching allows to reduce the delays.

When _writing_ we distinguish two different behaviors.
- **Write back cache**: the drive reports that writes are _complete after they have been cached_. This approach allows to reduce also write delays, but can be dangerous when using non-persistent caching, since, if power goes off, we would loose the cached data which hasn't been written to disk yet.
- **Write through cache**: the drive reports that writes are _complete after they have been written to the disk (and to the cache)_.

---

#### Operations delay for a HDD

The **total delay** of operations performed on a HDD is the sum of _4 components_:
1. **Rotational delay**: it is the time required to rotate the platter as to place the desired sector under the read head, _assuming that the head is on the right track_.
2. **Seek delay**: it is the time required to move the read head to the desired track;
3. **Transfer time**: it is the time required by the actual reading or writing of the desired amount of bytes;
4. **Controller overhead**: it is the overhead due to requests management.

**Remark**: sectors are numbered in such a way that successive sectors which are on the same platter but on different tracks are _skewed_ to take into account the angle by which the platters rotate during the seek delay of moving the head from one track to the adjacent one. This makes sequential reads and writes more efficient.

##### Rotational delay

We call **full rotation delay** $R$ the time required for a full revolution of the platters. Said $\omega$ the platters angular velocity in RPMs,
$$
R = \frac{1 \text{ revolution}}{\omega} \text{ [ minutes ]}.
$$

---

We can make a conservative approximation of the rotational delay by considering the case in which the desired track is on the opposite side of the platter w.r.t. the head. In such a case we have two make an half revolution of the platter, that is:
$$
T_{\text{rotation}} = \frac{R}{2}.
$$

##### Seek delay

The seek delay is NOT linear w.r.t. the number of tracks the head has to traverse.
Indeed the movement of the head during seek is composed of 4 phases:
- Acceleration;
- Coasting (_at constant speed_);
- Deceleration;
- Settling.

In practice a rough approximation of the average seek delay is obtained by dividing by 3 the time it takes to move the head from the outermost to the innermost track (_or vice-versa_), which we denote with $T_{\text{seek (MAX)}}$. That is:
$$
T_{\text{seek (AVG)}} \approx \frac{T_{\text{seek (MAX)}}}{3}.
$$

---

##### Transfer time

It is the time during which the data is either read from or written to the surface. It includes the time for the head to pass on the sectors. Hence it depends on the _revolution speed_ and on the _storage density_.

##### Service time and response time of an HDD

Through the quantities that we have described we can calculate the **service time** and the **response time** (_see the summary on "Performance"_) of the disk.
$$
T_{\text{service}} = T_{\text{rotation}} + T_{\text{seek}} + T_{\text{transfer}} + T_{\text{controller}};
$$
$$
T_{\text{response}} = T_{\text{service}} + T_{\text{queue}}
$$
where $T_{\text{queue}}$ is the time that the read or write request stays in queue, waiting to be processed.

##### Service time for contiguous reads/writes

The expression for the service time that we derived considers only the very pessimistic case where sectors are fragmented on the disk in the worst possible way.
In this scenario every access to the disk requires to "pay" $T_{\text{rotation}}$ and $T_{\text{seek}}$.
In many circumstances, this is NOT the case: _files are larger than one sector_ and _stored in a contiguous way_.

---

We can measure the **data locality** $L$ of a disk as the percentage of blocks which do not require rotational or seek delay to be accessed (_assuming that the head is at the previous block of the file to which they belong_).
In this setting the service time becomes:
$$
T_{\text{service}} = (1 - L) (T_{\text{seek}} + T_{\text{rotation}}) + T_{\text{transfer}} + T_{\text{controller}}.
$$

**Remark**: $T_{\text{seek}}$ and $T_{\text{rotation}}$ are the total seek delay and rotational delay respectively for all the sectors that we have to access.

#### Disk scheduling

We want to answer the following problem: suppose that we have a queue of requests to the disk, in which way should we reorder them in order to improve performance?
There are several algorithms which solve this problem:
- **First Come, First Served** (**FCFS**);
- **Shortest Seek Time First** (**SSTF**);
- **SCAN**;
- **C-SCAN**, **C-LOOK**, etc. .

##### First Come, First Served

FCFS is the most basic scheduler: it serves the requests in order without performing any optimization. It spends a lot of time seeking.

---

##### Shortest Seek Time First

SSTF tries to minimize the total seek time by accommodating the request in the queue whose initial block (to be read or written) is the closest w.r.t. the current position of the head.

**Pros**: it is _NOT optimal_ (_the greedy policy can zig-zag, which is never optimal_) but in general very good and easily implemented.

**Cons**: it is prone to _starvation_.

##### SCAN

The head _sweeps_ across the disk (going from the first sector to the last and then from the last to the first) serving requests as their first block (_to be read or written_) is encountered.

**Pros**: it has _good performance_, and _NO starvation_.

**Cons**: average access times are higher for requests at high and low addresses.

##### Circular-SCAN (C-SCAN)

It is _like SCAN_ but serves requests only when moving from the first sector to the last. That is, once it reaches the last sector, it goes back to the first without serving any request.

---

**Pros**: it is fairer than SCAN (_no issues with requests at high and low addresses_).

**Cons**: it has worse performance than SCAN.

##### C-LOOK

It is a variant of C-SCAN in which the head, instead of reaching the last sector, stops after having served the request with the initial block with highest address, and, instead of going back to the first sector (_without serving any request_), it stops when it reaches the initial block of the requests at the lowest address.

---

### Solid State Drive (SSD)

A **SSD** is a storage device which, unlike HDDs, has _NO mechanical or moving parts_.
It is built out transistors, but retains information despite power loss (unlike RAM).
A controller is included in the device with one or more solid state components.
It uses traditional HDD interfaces (protocol and physical connectors) and form factors, but it has higher performance.

#### Flash chips

SSDs relies on **flash chips** (_more specifically_, NAND based flash chips)  which are designed to store one or more bits in a single transistor: the level of charge trapped in the transistor is mapped to a binary value.
We can distinguish between:
- **Single-level cell** (**SLC**): single bit per cell;
- **Multi-level cell** (**MLC**): two bits per cell;
- **Triple-level cell** (**TLC**): three bits per cell;
- **QLC**, **PLC**, ... .

Flash chips are organized into **banks** which consist of a large number of _cells_.
A bank is accessed in two different sized units:
- **blocks**, which are typically of size 128 KiB or 256 KiB, and

---

- **pages**, which are few KiB in size (e.g. 4 KiB).

Within each bank there are a large number of blocks, within each block, there are a large number of pages (e.g. 64).

**Remark**: block/page terminology in the SSD context clashes with the usual one.

Given this flash organization, there are three low-level operations that a flash chip supports:
- **Read <u>a page</u>**.

- **Erase <u>a block</u>**: _before writing to a page within a flash, the nature of the device requires that you first **erase** the entire block the page lies within_. Erase destroys the contents of the block, _by setting each bit to 1_, therefore you must be sure that any data you care about in the block has been copied elsewhere before executing the erase.

- **Program <u>a page</u>**: _once a block has been erased_, the program command can be used to change some of the 1s within a page to 0s, writing the desired content.

In a flash chip _each page_ has a _state_ associated with it:
- **INVALID**: is the initial state for each page;
- **ERASED**: it is the state to which all pages in a block are set after such block has been erased; 
- **VALID**: it is the state to which transitions and _ERASED_ page after it has been programmed.

---

**Remark**: we can restate what we explained earlier: it is possible to program only pages in the _ERASED_ state, and it makes sense to read only pages in the _VALID_ state.

#### Flash Transition Layer (FTL)

We want to understand how to run the flash chips that we've described into something that looks like a typical storage device. The standard storage interface is a simple block-based one, where (_logical_) blocks of size 512 B (for example) can be read or written, given a (_logical_) block address.
The **task of the flash-based SSD** is to _provide_ that _standard block interface_ _atop the raw flash chips_ inside it. The **FTL** is responsible exactly of this functionality: it takes read and write requests on _logical blocks_ and turns them into low-level read, erase, and program commands of the underlying _physical blocks_ and _physical pages_.
The FTL should accomplish this task with the goal of delivering excellent performance and high reliability.

<div class="definition">

In particular, one important goal is to reduce as possible **write amplification**, which is defined as the total write traffic (_in bytes_) issued to the flash chips by the FTL, divided by the total write traffic (_in bytes_) issued by the client to the SSD.

</div>

---

The main concern for what regards reliability is **wear out**: when a flash block is erased and programmed, it slowly accumulates a little bit of extra charge; over time it becomes increasingly difficult to distinguish between 0 and 1 and the extra charge builds up.
To handle this problem, FTL should try to spread writes across the blocks of the flash as evenly as possible, ensuring that all the blocks of the device wear out at roughly the same time. This is known as **wear leveling**.

##### Log-structured FTL

In order to minimize write amplification and wear out, today, most FTLs, are **log-structured FTLs**. They work as follows: upon a write to a logical block $N$, the device appends the write to the next free spot in _currently-being-written-to_ block; we call this _style of writing_ **logging**. To allow for subsequent reads of block $N$, the device keeps a **mapping table** (both in its memory and persistent, in some form, on the device); this table stores the physical address of each logical block in the system.

This approach to FTL has still some problems.
- The overwrites of logical blocks lead to **garbage** (_since we keep programming new pages without erasing the old blocks_). The device has to perform periodically **garbage collection** (**GC**) to find blocks that should be erased and free space for future writes.
- In-memory mapping tables have an high cost which increases with the capacity of the device.

---

###### Garbage collection

The basic GC process is simple: find a block that contains one or more garbage pages, read in the live (non-garbage) pages from that block, write out those live pages to the log, and (finally), reclaim the entire block for use in writing.

Observe that GC can be expensive: its cost depends on the amount of data in the blocks that has to be migrated. To alleviate this problem usually SSDs are over-provisioned by adding extra flash capacity, which allows to delay cleaning, and GC is executed in background during less busy periods for the disk.

Another problem of GC is that, in order to perform it efficiently, the SSD has to know which pages are invalid. The issue is that most file systems don't explicitly delete data, they just remove the corresponding meta-data. Indeed, as we anticipated, explicit deletion is inefficient for HDDs. This problem is solved by the introduction of a new storage API command: **trim**, which allows to specify that a contiguous set of _logical blocks_ have been deleted. This is useless for HDD, but, because of what we just explained, can significantly improve the efficiency of GC in SSDs since we don't have to copy useless pages.

###### Mapping table size

Observe that, in a 1 TiB SSD, with 4 KiB pages, a mapping table with one 4 B entry for page requires 1 GiB of space.

---

Hence, _page level mapping tables_ are _impractical_.

- **Block Based Mapping**

One approach to reduce the costs of mapping is to only keep a pointer per _block_ of the device, instead of per page, reducing the amount of mapping information.

In block-level mapping, the address mapping is less straightforward than the page-level case. Specifically, we think of the logical address space of the device as being chopped into chunks that are the size of the physical blocks within the flash. Thus, the logical block address consists of two portions: a chunk number and an offset. FTL computes the address of the desired flash page by adding the offset from the logical address to the physical address of the block, retrieved from the map through the chunk number.

This mechanism makes clear the biggest issue of block mapping: small writes (i.e. writes of data smaller than a physical block in size). When we do a small write, since all the data in a logical chunk must stay in the same physical block, we need to copy the whole block into a new one, modifying only the desired pages. This can greatly degrade the performance.

- **Hybrid Mapping**

To enable flexible writing but also reduce mapping costs, many modern FTLs employ a **hybrid mapping** technique. With this approach, the FTL keeps a few blocks erased and directs all writes to them.

---

These are called **log blocks**. Because the FTL wants to be able to write any page to any location within the log block without all the copying required by a pure block-based mapping, it keeps _per-page_ mappings for these log blocks.
The FTL has thus two types of mapping tables in its memory; a small set of _per-page_ mappings called _log table_, and a larger set of _per-block_ mappings called _data table_. When looking for a particular logical block, the FTL will first consult the log table; if the logical block's location is not found there, the FTL will then consult the data table to find its location and then access the requested data.

- **Page mapping plus caching**

Given the complexity of the hybrid approach above, others have suggested simpler ways to reduce the memory load of page-mapped FTLs. The simplest is just to cache only the active parts of the FTL in memory, thus reducing the amount of memory needed.
This approach can work well, for example, if a given workload only accesses a small set of pages. Of course the approach can also perform poorly if memory cannot contain the working set.

###### Wear leveling

The basic log-structuring approach does a good initial job of spreading out write load, and garbage collection helps as well. However, sometimes a block will be filled with long-lived data that does not get over-written;

---

in this case, garbage collection will never reclaim the block, and thus it does not receive its fair share of write load. To remedy this problem, the FTL must periodically read all the live data out of such blocks and re-write it elsewhere, thus making the block available for writing again. This process of wear leveling increases the write amplification of the SSD, and thus decreases performance as extra I/O is required to ensure that all blocks wear at roughly the same rate.

#### Comparison with HDDs

We can measure the reliability of an SSD and compare it with HDDs through the following two metrics:
- **Unrecoverable Bit Error Ratio** (**UBER**): it is the average number of data errors per bit read;
- **TeraBytes Written** (**TBW**): it is the number of TiB that we can write to a drive while still meeting the requirements.

If we were to plot the number of corrupted sectors over the number of full rewrites of the drive, we would observe that for HDD the growth of this quantity is linear. For SSD instead, at the beginning we have a flat line, which after a certain number of rewrites start growing very fast till it intersects and overtakes the HDD's curve.
At this intersection point we can measure the TBW from the number of drive rewrites, and the UBER as the fraction of corrupted sectors.

---

Flash cells can accept data recording between 3,000 and 100,000 during their lifetime. Once the limit value is exceeded, the cell "forgets" any new data.
A typical TBW for a 250 GB SSD is between 60 and 150 Terabytes of data written to the drive
It is difficult to comment on the duration of SSDs, since it is influenced by many factors.

---

### Redundant Array of Independent Disks (RAID)

**RAID** is a technique which allows to use multiple independent disks in concert to build a faster, bigger, and more reliable disk system. It is in contrast w.r.t. JBOD (Just a Bunch Of Disks) method, where each disk is a separate device with a different mount point.
Indeed, a RAID system is _transparent_ to the outside and can be used as if it were a single disk. But internally it is a complex system: disks are managed by a dedicated CPU and special software; there is also a RAM and other non-volatile memory (_other than the disks_).

#### Striping vs Redundancy

RAID relies on two orthogonal techniques:
- **Striping**: to improve _performance_;
- **Redundancy**: to improve _reliability_.

In **striping** data (a file, a vector, a table, ...) are written sequentially in units, on multiple disks, according to a cyclic algorithm (round robin).

<p align="center">
    <img src="static/raid-striping.svg"
    width="400mm" />
</p>

---

We define **stripe unit** the dimension of the unit of data that are written on a single disk. The **stripe width** is the number of disks considered by the striping algorithm.
Striping allows to parallelize I/O requests.
- _Multiple I/O requests_ will be executed in parallel by many disks, _reducing the queue length_.
- _Single multi-block I/O requests_ will be executed in parallel by many disks, _increasing the transfer rate_.

But, the larger is the stripe width, the higher is the probability of failure for the whole system. This is the main motivation for the introduction of redundancy.

**Redundancy** is achieved through _error correcting codes_ or by _mirroring_, in both cases we sacrifice a bit the performance of the system, to gain in reliability: we can tolerate the failure of a certain number of disks.

#### RAID levels

RAID offers many possible configurations, known as **levels**, which differ in the way **striping** and **redundancy** are exploited and mixed together.
We will compare the performance and reliability of the different levels. For what regards the performance, we will distinguish between two workloads: sequential I/O and random I/O, denoting with $S$ and $R$ the throughput of the single disk in the sequential and random access respectively.

---

**Remember**: the difference between the two is that in sequential access we need to pay the seek and rotational delay once.

Furthermore, we will consider also the total capacity of the system, and we'll denote with $N$ the number of disks, and with $B$ the capacity of a single disk.

##### RAID 0

In **RAID 0** we _only apply striping_. Data is split among the disks following a striping algorithm. RAID 0 is used only when performance and capacity are more important than reliability. As previously remarked, in this setting a single disk failure results into data loss.
To maximize the performance of RAID 0, it is important to choose an appropriate _stripe unit_ a.k.a. _chunk size_:
- small chunk size allows to maximize the parallelism when accessing a single file, with the con that we have to pay the maximum seek delay among all the disks in which the file has been split;
- big chunk size instead maximizes the parallelism when there are multiple I/O requests to different files, in this case, for each file, we pay the seek delay for only one disk.

**Reliability**: 0, _if any drive fails, data is permanently lost_.

**Sequential access**: $N S$, _we have full parallelization among drives_.

---

**Random access**: $N R$, _we have full parallelization among drives_.

**Capacity**: $N B$, all drives are used for storage.

##### RAID 1

In **RAID 1** we _only apply mirroring_. The same data is replicated among all the disks. This requires at least two disks, and in practice it is never done with more than two disks because of the overhead and the costs which are too high w.r.t. the increase in reliability.

**Reliability**: $N-1$ (_remember that only in theory we have $N > 2$_), _each drive is a copy of the other, unless every drive fails, we won't loose data_.

**Sequential read**: $N S$, _we can parallelize reads across drives_.

**Sequential write**: $S$, _we have to write the same data on all the drives_.

**Random read**: $N R$ (_analogous to sequential_).

**Random write**: $R$ (_analogous to sequential_).

**Capacity**: $B$, all the drives store the same data.

**Important remark**: _mirrored writes_ should be **atomic**: all copies are written or none is written. However this is hard to guarantee (think about power failures). For this reason, many RAID controllers include a **write-ahead log**,

---

which is a battery backed, non-volatile storage of pending writes. Thanks to a specific procedure we can recover the out-of-sync writes.

##### RAID 0+1

In **RAID 0+1** a RAID 1 manages $M$ drives (remember that in practice $M = 2$), 

each of which is a RAID 0 which manages $L$ HDDs, instead of being a simple HDD.
That is, $N = M L$.

**Reliability**: $M-1$, in practice $1$ since $M = 2$ (_remember that each RAID 0 fails after the first failure of one of its HDDs_).

The throughput of a sequential read in this configurations varies greatly depending on how we distribute the load among the disks. To understand why, let's consider an example.

<p align="center">
    <img src="static/raid0+1.svg"
    width="600mm" />
</p>

---

The picture above represents a RAID 0+1 with $M = 2$, $L = 2$ ($N = 4$), $B = 4$.
Suppose that we want to retrieve the data corresponding to the whole block sequence 0, 1, ..., 7. This is clearly a sequential read.

Let's consider two ways of splitting this sequential read among D0, D1, D2, and D3.

In the first approach we can split the sequence of blocks at every level according to a cyclic algorithm, as it is done in striping, with a stripe width of 2 blocks the first time, and of 1 the second. That is, the root RAID assigns the reading of 0, 1, 4, 5 and 2, 3, 6, 7 to the RAID 0 on the left and on the right respectively. The RAID 0 on the left assigns the reading of 0, 4 to D0 and the reading of 1, 5 to D1. Analogously the RAID 0 on the right assigns the reading of 2, 6 to D2 and the reading of 3, 7 to D3. Now let's consider what happens when D0 reads 0, 4: since this is a sequential read, we need to consider the time spent when going through 2, even if we're not reading. That is, at the end we have 2 read blocks and 1 wasted block. This happens to all the other disks analogously. Observe that, if we keep doubling the length of the sequence of blocks to read (and we increase $B$ accordingly), the number of wasted blocks is always 1 less than the number of read blocks. Hence (_asymptotically_) we're halving the throughput of the system, which results in $\frac{N}{2} S$ even though we're using all the $N$ drives in parallel.

---

In the second approach, instead, we can split the sequence of blocks at each level consistently with the type of RAID at that level. That is, since the first RAID is a RAID 1, we split the sequence into the first and second half, assigning 0, 1, 2, 3 to the RAID 0 on the left, and 4, 5, 6, 7 to the RAID 0 on the right.

Then, since the RAIDs at the second level are RAID 0, we use a cyclic algorithm (with stripe width of 1), assigning 0, 2 to D0, 1, 3 to D1, 4, 6 to D2, and 5, 7 to D3.
In this way we have no wasted blocks, and this holds even if we keep doubling the length of the sequence of blocks to read. Hence we exploit the full throughput of the system, which is $N S$. Observe that this is also consistent with the results we obtained for sequential reads in RAID 1 and RAID 0 individually. Each RAID 0 at the second level has a throughput of $L S$. Then, the RAID 1 at the first level (remember that the RAID 0 below are transparent, considering the RAID management overhead negligible) have a throughput of $M L S = N S$.

**Unfortunately**, for reasons to me unknown, on the slides we considered only the unintuitive and inefficient way of distributing the sequential read over the disks, hence we will consider a throughput of $\frac{N}{2} S$.

**Sequential read**: $\frac{N}{2} S$.

**Sequential write**: $L S = \frac{N}{M} S$, in practice $\frac{N}{2} S$ (_see write in RAID 1_).

---

Of course the way in which we split the load for random reads is immaterial. In every case we get full throughput.

**Random read**: $N R$.

**Random write**: $L R = \frac{N}{M} R$, in practice $\frac{N}{2} R$.

**Capacity**: $L B = \frac{N}{M} B$, in practice $\frac{N}{2} B$.

##### RAID 1+0

In **RAID 1+0** a RAID 0 manages $L$ drives, each of which is a RAID 1 which manages $M$ (_in practice, $M = 2$_) HDDs. Again, $N = L M$.

**Reliability**: from $M-1$, up to $L (M-1)$; in practice from $1$ to $\frac{N}{2}$ drives can fail without making the full system fail.

The other quantities of interest are the same w.r.t. RAID 0+1. For this reason RAID 1+0 is _preferred_ over RAID 0+1 since it has the same performance but it is (_a bit_) more reliable.

##### RAID 4

RAID 1 offers highly reliable data storage, but it allows to use only half of the array capacity for storage. This motivates the introduction of **RAID 4** which exploits information coding techniques to build light-weight error recovery mechanisms which offer the same reliability of RAID 1. In particular, in RAID 4, one of the $N$ drives is a **parity drive**.

---

That is, the $i$-th bit of the parity drive is such that if we consider the set of bits in position $i$ of all the $N$ drives, there is an even number of 1s. Of course, the minimal accessible unit is still the block (but this does not compromise the property just stated). In particular, said ${DJ}_i$ the $i$-th bit of disk $J \in \{ 1, \ldots, N-1 \}$, the parity bit $P_i$ is easily computed as:
$$
P_i = {D1}_i \oplus \ldots \oplus {D(N-1)}_i
$$
(_this is easy to see thinking about how the XOR is defined_).
This way of computing the parity bit is known as **additive parity**.
Observe that, if we just need to modify one bit in a disk and the parity bit accordingly, we can use **subtractive parity**:
$$
P_i^{\text{(new)}} = P_i^{\text{(old)}} \oplus ({DJ}_i^{\text{(old)}} \oplus {DJ}_i^{\text{(new)}}).
$$
This follows from the fact that $P_i^{\text{(new)}} = P_i^{\text{(old)}}$ if ${DJ}_i^{\text{(old)}} = {DJ}_i^{\text{(new)}}$, otherwise we need to flip it.

Observe that, if any drive were to fail, we could recover its content through the additive parity formula, swapping $P_i$ with the failed ${DJ}_i$ (_of course we can also recompute the content of the parity drive leaving the formula as is_).

**Reliability**: $1$, _because of what we just remarked_.

**Sequential read**: $(N-1) S$: _we can parallelize the reads over all the storage drives_.

---

**Sequential write**: $(N-1) S$. When performing a sequential write we can assume that we'll overwrite all the ${D1}_i, \ldots {D(N-1)}_i$ bits together, for several contiguous $i$s. Hence we can compute $P_i$ without having to read anything. Then we can parallelize the writing of data onto ${D1}, \ldots, {D(N-1)}$ and the writing of the parity bits on $P$.

**Random read**: $(N-1) R$, _analogous to sequential read_.

**Random write**: $\frac{R}{2}$. Each random write requires the reading of ${DJ}_i$ and $P_i$ in order to apply the subtractive parity formula, plus the two writes of the new values. Hence, in this case to each random write corresponds 1 read and 1 write of the parity drive which we cannot parallelize (_in the sequential case, since we distribute the data in a cyclic way, we need 1 write of the parity drive every $N-1$ writes of the storage drives_). Then the parity drive is the bottleneck: for every request we need to access it two times in a serialized fashion.

**Capacity**: $(N-1) B$.

##### RAID 5

**RAID 5** is an improvement w.r.t. RAID 4 in which there is no dedicated parity drive, the parity bits are spread cyclically over all the drives.

**Reliability**: $1$ (_analogous to RAID 4_).

---

**Sequential read**: $(N - 1) S$. In this case we can parallelize the read over all the $N$ drives, but, since the read is sequential, we need to go through also the parity blocks which are NOT of interest when reading and constitute $\frac{1}{N}\text{th}$ of the read blocks on average.

**Sequential write**: $(N - 1) S$ (_analogous to RAID 4, they only thing which changes is where we write the parity bit_).

**Random read**: $N R$. In this case we can parallelize the reading over all drives.

**Random write**: $\frac{N}{4} R$. In this case, as in RAID 4, each request requires 4 accesses, but (on average) we can distribute them over all the drives (instead, in the RAID 4 case, we had a bottleneck caused by the fact that half of the times we had to access the parity drive).

**Capacity**: $(N-1) B$.

##### RAID 6

**RAID 6** has two parity blocks per stripe, instead of one as in RAID 4 and RAID 5. It uses Solomon-Reeds codes with two redundancy schemes. It can tolerate up to two failures, but each write requires 6 accesses to the disks.

**Reliability**: $2$.

**Sequential read**: $(N-2) S$.

**Sequential write**: $(N-2) S$.

---

**Random read**: $N R$.

**Random write**: $\frac{N}{6} R$.

**Capacity**: $(N-2) B$.

#### Other considerations about RAIDs

##### Hot spare

Many RAIDs include an **hot spare** which is an unused drive installed in the system which stays in idle until one of the disk fails. Then the hot spare is automatically used to replace the failed disk.

##### Hardware vs Software implementation

RAIDs can be implemented both in hardware and software.
- Hardware implementation is faster and more reliable.
- But migrating an hardware RAID array to a different hardware controller almost never works.

#### RAID reliability computations

We can compute the $MTTF$ (_see Dependability summary_) of the different RAID levels through the following approximation:
$$
\mathbb{P}(\text{failure}) \approx \frac{1}{MTTF}.
$$

---

In particular, we have to apply it both to the whole system, and to the single disks.
If follows that:
$$
{MTTF}_{\text{RAID}} \approx \frac{1}{\mathbb{P}(\text{RAID failure})}.
$$
Let's examine in depth this expression for each RAID level.

- **RAID 0**

$$
\mathbb{P}(\text{RAID 0 failure}) = \mathbb{P}(\text{1st disk failure}) \approx
$$
$$
\approx N \mathbb{P}(\text{single disk failure}) \approx \frac{N}{{MTTF}_{\text{disk}}}.
$$

Then:
$$
{MTTF}_{\text{RAID 0}} \approx \frac{{MTTF}_{\text{disk}}}{N}.
$$
In this case it is easy to show that we have equality since the subsystems are in series (_see the Dependability summary_).

- **RAID 0+1**

$$
\mathbb{P}(\text{RAID 0+1 failure}) = \mathbb{P}(\text{1st failure}) \mathbb{P}(\text{2nd failure in other stripe group}) \text{.}
$$

$$
\mathbb{P}(\text{1st failure}) \approx \frac{N}{{MTTF}_{\text{disk}}}.
$$

Let $G$ be the number of drives in a stripe group. Usually $G = \frac{N}{2}$. Observe that the 2nd failure must happen before we repair the first failed drive (i.e. in $MTTR$).

---

$$
\mathbb{P}(\text{2nd failure in other stripe group}) \approx G \frac{MTTR}{MTTF_{\text{disk}}}.
$$

Then:
$$
{MTTF}_{\text{RAID 0+1}} \approx \frac{MTTF^2_{\text{disk}}}{N \ G \  MTTR}.
$$

- **RAID 1+0**

$$
\mathbb{P}(\text{RAID 1+0 failure}) = \mathbb{P}(\text{1st failure}) \mathbb{P}(\text{2nd failure in same mirror group}) \text{.}
$$

$$
\mathbb{P}(\text{1st failure}) \approx \frac{N}{{MTTF}_{\text{disk}}}.
$$

Usually we have only 2 drive for mirror group. Then:
$$
\mathbb{P}(\text{2nd failure in same mirror group}) \approx \frac{MTTR}{{MTTF}_{\text{disk}}} .
$$

Then:
$$
{MTTF}_{\text{RAID 1+0}} \approx \frac{MTTF^2_{\text{disk}}}{N \  MTTR}.
$$

- **RAID 4**

$$
\mathbb{P}(\text{RAID 4 failure}) = \mathbb{P}(\text{1st failure}) \mathbb{P}(\text{2nd failure}).
$$

$$
\mathbb{P}(\text{1st failure}) \approx \frac{N}{{MTTF}_{\text{disk}}}.
$$

---

$$
\mathbb{P}(\text{2nd failure}) \approx \frac{(N-1) MTTR}{MTTF_{\text{disk}}}.
$$

Then:
$$
{MTTF}_{\text{RAID 4}} \approx \frac{{MTTF}^2_{\text{disk}}}{N(N-1) \ MTTR}.
$$

- **RAID 5**

Same as RAID 4:
$$
{MTTF}_{\text{RAID 5}} \approx \frac{{MTTF}^2_{\text{disk}}}{N(N-1) \ MTTR}.
$$

- **RAID 6**

$$
\mathbb{P}(\text{RAID 4 failure}) = \mathbb{P}(\text{1st failure}) \mathbb{P}(\text{2nd failure}) \mathbb{P}(\text{3rd failure}).
$$

$$
\mathbb{P}(\text{1st failure}) \approx \frac{N}{{MTTF}_{\text{disk}}}.
$$

$$
\mathbb{P}(\text{2nd failure}) \approx \frac{(N-1) MTTR}{MTTF_{\text{disk}}}.
$$

Observe that the 3rd failure has to happen before we repair either of the two drives. And, analogously to $MTTF$ of a series of drives, the mean time to repair either of the two drives is $\frac{MTTR}{2}$. Hence:

$$
\mathbb{P}(\text{3rd failure}) \approx \frac{N-2}{MTTF_{\text{disk}}} \frac{MTTR}{2}.
$$

---

Then:
$$
{MTTF}_{\text{RAID 6}} = \frac{2 {MTTF}^3_{\text{disk}}}{N(N-1)(N-2) \ {MTTR}^2} .
$$