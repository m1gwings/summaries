---
marp: true
theme: summary
math: mathjax
---
# Building infrastructures

<div class="author">

Cristiano Migali
(_adapted from the slides of Prof. Manuel Roveri_)

</div>

<style>
section {
    font-size: x-large;
}

.definition {
    padding-left: 0.5cm;
    padding-right: 0.5cm;
    background: var(--algorithms);
    border-radius: 0.5cm;
    border-style: solid;
    border-color: var(--text);
    border-width: 3pt;
    text-align: justify;
}
</style>

## Building basics

At a high level, a data-center building has multiple components.
- There is a **mechanical yard** that hosts all the cooling systems, such as _cooling towers_ and _chillers_ (_see later_).
- There is an **electrical yard** that hosts all the electrical equipment, such as generators and power distribution centers.
- The **server hall** hosts the compute, storage, and networking equipment. As we've already seen, these devices are installed inside racks, which are arranged in rows, forming several aisles. We can classify these aisles into _cold aisles_ and _warm aisles_. In particular, the front side of the equipment faces a cold aisle, where cold air is injected from the floor. This air goes through the devices and exits from their back side once it is warm, ending in a warm aisle.

---

## Data-center power systems

Power, directed to the data-center, enters first a utility substation which transforms high voltage to medium voltage. Medium voltage is used for site-level distribution to the primary distribution centers. From here, the power enters the building with the low-voltage lines going to the **Uninterruptible Power Supply** (UPS) systems. The UPS also takes a second feed at the same voltage from a set of diesel generators that cut in when utility power fails.

The UPS typically combines three functions in one system.
- First, it contains a transfer switch that chooses the active power input (either utility power or generator power). After a power failure, the transfer switch senses when the generator has started and is ready to provide power; typically, a generator takes 10-15 s to start and assume the full rated load.
- Second, it contains some form of energy storage to bridge the time between the utility failure and the availability of generator power.
- Third, it conditions the incoming power feed, removing voltage spikes or sags, or harmonic distortions in the AC feed. This conditioning can be accomplished via "double conversion".

In any case, the outputs of the UPS system are routed to the data center floor, where they are connected to **Power Distribution Units** (**PDUs**).

---

PDUs are the last layer in the transformation and distribution architecture and route individual circuits to the computer cabinets.

## Data-center cooling systems

Data center cooling systems remove the heat generated by the equipment. To remove heat, a cooling system must employ some _hierarchy of loops_, each circulating a _cold medium_ that warms up via some form of heat exchange and is somehow cooled again. In particular, we can distinguish between **open loops**, where the outgoing warm medium is replaced with a cool supply from the outside, so that each cycle through the loop uses new material, and **closed loops**, in which the medium recirculates continuously, transferring heat to either another loop, via heat exchangers, or to the environment. All systems of loops must eventually transfer heat to the outside environment.

The simplest topology is fresh air cooling (or air economization): essentially, opening the windows.

Closed-loop systems come in many forms, the most common being the air circuit on the data-center floor. Its function is to isolate and remove heat from the servers and transport it to a heat exchanger. In particular, cold air flows from the floor to the servers, heats up, and eventually reaches a heat exchanger to cool it down again for the next cycle through the servers.

---

Typically, data-centers employ _raised floors_: concrete tiles installed onto a steel grid two to four feet above the slab floor, resting on supports. The underfloor area often contains power cables to racks, but its primary purpose is to distribute cool air to the server racks. The air flows through the underfloor plenum, the racks, and back to the **Computer Room Air Conditioning** (**CRAC**), which are special rooms in the data-center, devoted to air conditioning (as the name suggests).

The simplest closed-loop systems contain _two loops_. The first is the floor to racks to CRAC loop that we just described. The second loop leads directly from the CRAC to external heat exchangers that discharge the heat to the environment.

There are also closed-loop systems that involve _three loops_.
The first loop is the usual _data-center floor loop_; it is important to remark that, in this three loops system, the air going through the first loop is cooled by fan coils. The second loop is known as _process loop_: warm water from the fan coils returns to the cooling plant to be chilled and pumped back to the fan coils.
Finally, the third loop is the _condenser water loop_ which removes heat received from the process water through a combination of mechanical refrigeration by chiller units and evaporation in cooling towers.

---

### Chillers

A water-cooled **chiller** can be thought as a water-cooled air conditioner.

### Cooling towers

**Cooling towers** cool a water stream by evaporating a portion of it into the atmosphere. They do not work as well in very cold climates because they need additional mechanisms to prevent ice formation

### Free cooling

**Free cooling** refers to the use of cold outside air to either help produce chilled water or directly cool servers. It is not completely free in the sense of zero cost, but it involves very low-energy costs compared to chillers.

### Comparison between the different cooling topologies

Each topology presents tradeoffs in complexity, efficiency, and cost.
- **Fresh air cooling** can be very efficient but does not work in all climates, requires filtering of airborne particulates, and can introduce complex control problems.

---

- **Two-loop systems** are easy to implement, relatively inexpensive to construct, and offer isolation from external contamination, but typically have lower operational efficiency.
- A **three-loop system** is the most expensive to construct and has moderately complex controls, but offers contaminant protection and good efficiency.

### In-rack, in-row, and liquid cooling

- **In-rack coolers** add an air-to-water heat exchanger at the _back of a rack_, so the hot air exiting the servers immediately flows over coils cooled by water, essentially reducing the path to the CRAC input.

- **In-row cooling** works like in-rack cooling except the cooling coils are not in the rack, but _adjacent to the rack_.

- **Liquid cooling** consists in directly _cooling server components using cold plates_, i.e. local liquid-cooled heat sinks. Since it is impractical to cool all compute components with cold plates, only the components with the highest power dissipation are targeted for liquid cooling.
The liquid circulating through the heat sinks transports the heat to a liquid-to-air or liquid-to-liquid heat exchanger that can be placed to the tray or rack, or be part of the data center building.

---

### Contained-based data centers

Container-based data centers go one step beyond in-row cooling by placing the server racks inside a container (typically 6 to 12 m long) and integrating heat exchange and power distribution into the container as well.

## Data-center power consumption

