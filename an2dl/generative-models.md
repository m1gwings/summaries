---
marp: true
theme: summary
math: mathjax
---
# Generative models for images

<div class="author">

Cristiano Migali

</div>

- The **goal** of **generative models for images** is, given a training set of images $\mathcal{D} = \{ I_i \}$, to generate other images that are similar to those in $\mathcal{D}$.

Images live in a very "difficult to describe" manifold in a huge dimensional space.

## Why do we need generative models?

Generative models can be used for **data augmentation**, for simulation, for inverse problems like super-resolution, in-painting, and colorization.  Training generative models can also enable inference of latent representations that can be useful as **general features**.

Modeling the distribution of natural images is known as the "holy grail" of image processing. It can be avery useful regularization prior in other problems or to perform anomaly detection.

On top of specific applications of image generation, the first effective generative model (i.e. GANs) give rise to new training paradigm and practices.

On top of this, nowadays we all know how realistic generative models are and their use in everyday life.

## Generative Adversarial Networks

- **Generative Adversarial Networks** (**GANs**) are a family of generative models. They were born for image classification.

In the **GAN** approach, instead of trying to explicitly model the density $\phi_\underline{s}$ describing the manifold of natural images, we just find out a model able to generate samples that "look like" training samples $S$.

Instead of sampling from $\phi_\underline{s}$, we **sample a seed** from a known distribution $\phi_\underline{z}$ which is defined a-priori. The samples from this distribution are also referred to as **noise**. The seed is fed to a learned transformation that generates realistic samples, as if they were drawn from $\phi_\underline{s}$.

In particular this transformation is learned by a neural network. This neural network is going to be **trained in an unsupervised manner**: **no label is needed**.

The biggest challenge is to define a suitable loss for assessing whether the output is a realistic image or not.

---

The GAN solution is to resort to another neural network to define the loss.

In particular, we train a pair of neural networks addressing two different tasks that compete in a sort of **two players** (adversarial) **game**.
These models are:
- a **generator** $\mathcal{G}$ that produces realistic samples, e.g. taking as input some random noise;
- a **discriminator** $\mathcal{D}$ that takes as input an image and **assess whether it is real or generated by $\mathcal{G}$**.

We train the two and at the end, we keep only $\mathcal{G}$.

$\mathcal{G}$ is trained to generate images that can fool $\mathcal{D}$, namely can be classified as "real" by $\mathcal{D}$. The loss of $\mathcal{G}$ is therefore given by $\mathcal{D}$.

At the end of the training, we hope $\mathcal{G}$ to succeed in fooling $\mathcal{D}$ consistently. We discard $\mathcal{D}$ and keep $\mathcal{G}$ as generator.
$\mathcal{D}$ is expected to be effective in distinguishing real from fake images at the end of the training, thus, if $\mathcal{G}$ can fool $\mathcal{D}$, it mean that $\mathcal{G}$ is a generator.

Both $\mathcal{D}$ and $\mathcal{G}$ are conveniently chosen as FFNNs or CNNs.
In particular: $\mathcal{D}(\cdot, \theta_d) : \underline{s} \mapsto \mathcal{D}(\underline{s}, \theta_d)$ and $\mathcal{G}(\cdot, \theta_g) : \underline{z} \mapsto \mathcal{G}(\underline{z}, \theta_g)$ where $\theta_d$ and $\theta_d$ are network parameters, $\underline{s} \in \mathbb{R}^n$ is an input image (either real or generated by $\mathcal{G}$) and $\underline{z} \in \mathbb{R}^d$ is some random noise to be fed to the generator.

- $\mathcal{D}(\underline{s}, \theta_d) \in [ 0, 1 ]$ is the estimated probability that the input is a true image.
- $\mathcal{G}(\underline{z}, \theta_g) \in \mathbb{R}^n$ gives as output the generated image.

A good discriminator is such that:
- $\mathcal{D}(\underline{s}, \theta_d)$ is maximum when $\underline{s} \in S$ (i.e. $\underline{s}$ is a true image from the training set);
- $1-\mathcal{D}(\underline{s}, \theta_d)$ is maximum when $\underline{s}$ is generated from $\mathcal{G}$, and thus $1-\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d)$ is maximum when $\underline{z} \sim \phi_\underline{z}$.

Training $\mathcal{D}$ consists in maximizing the **binary cross-entropy**:
$$
\max_{\theta_d} \left( \mathbb{E}_{\underline{s} \sim \phi_\underline{s}} [\log \mathcal{D}(\underline{s}, \theta_d) ] + \mathbb{E}_{\underline{z} \sim \phi_\underline{z}} [\log(1-\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d))] \right).
$$

A good generator $\mathcal{G}$ makes $\mathcal{D}$ to fail, thus minimizes the above:
$$
\min_{\theta_g} \max_{\theta_d} \left( \mathbb{E}_{\underline{s} \sim \phi_\underline{s}} [\log \mathcal{D}(\underline{s}, \theta_d) ] + \mathbb{E}_{\underline{z} \sim \phi_\underline{z}} [\log(1-\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d))] \right).
$$

The optimization problem can be solved by an iterative numerical approach.
We **alternate**:
- $k$ steps of Stochastic Gradient Ascent w.r.t. $\theta_d$, keeping $\theta_g$ fixed;

---

- 1 step of SGD w.r.t. $\theta_g$, keeping $\theta_d$ fixed (observe that the first term in the loss does NOT depend on $\theta_g$).

During early learning stages, when $\mathcal{G}$ is poor, $\mathcal{D}$ can reject samples with high confidence because they are clearly different from the training data (thus $\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d) \approx 0$). In this region, $\log(1-\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d))$ is almost flat, thus has very low gradient. For this reason, when we optimize for $\theta_g$, instead of minimizing
$$
\log(1-\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d)),
$$
we maximize
$$
\log(\mathcal{D}(\mathcal{G}(\underline{z}, \theta_g), \theta_d)).
$$
This is equivalent in terms of loss function, but provides a stronger gradient during the early learning stages.

### Vector arithmetic

...

### Conditional GANs

### Anomaly detection for GANs
