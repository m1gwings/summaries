---
marp: true
theme: summary
math: mathjax
---
# Model selection

<div class="author">

Cristiano Migali
(_adapted from the slides of Prof. Marcello Restelli_)

</div>


## "No Free Lunch" theorems

We're going to discuss some theoretical limits to the ability of learning when the set of possible concepts to learn is unconstrained.

- In particular, in the context of a binary classification problem, a **concept** is a binary function $f : X \rightarrow \{ 0, 1 \}$ which specifies the true membership of each input. That is, it is the function that we want to learn.

Let $\mathcal{F}$ be the set of all possible concepts that our learner $L$ (an algorithm like the ones we have seen) could have to learn. We can denote with $\text{Acc}_G(L, f)$ the **generalization accuracy** of learner $L$ when trying to learn concept $f$, which is the accuracy on NON-training samples. Then, if $\mathcal{F}$ is unconstrained, that is: $\mathcal{F} = \{ 0, 1 \}^X$ (_the set of binary functions over $X$_), then the following result holds.
- **No Free Lunch theorem**: for any learner $L$, for any distribution $\mathcal{P}$ over the input set $X$ which generates the training set $\mathcal{D}$, for any training size $N$:
$$
\frac{1}{|\mathcal{F}|} \sum_{f \in \mathcal{F}} \text{Acc}_G(L, f) = \frac{1}{2}.
$$

That is, the average generalization accuracy of each learner $L$ is the same as the average generalization accuracy of the model which always provides random guesses.

> **Proof sketch**: given a training set $\mathcal{D}$, for every concept $f$ where $\text{Acc}_G(L, f) = 0.5 + \delta$ we can find a concept $f'$ where $\text{Acc}_G(L, f') = 0.5 - \delta$. In particular $f'$ is defined as follows:
$$
\forall \underline{x} \in \mathcal{D} \ f'(\underline{x}) = f(\underline{x}); \forall \underline{x} \not \in \mathcal{D} f'(\underline{x}) \neq f(\underline{x}).
$$
> Indeed, since the training set is the same (_and we assume that the learner is deterministic [this is not a strong assumption if we consider the training set as a tuple instead of a set since this would allow us to keep also track of the order by which the samples are presented]_) the hypothesis chosen by $L$ will be the same for both concepts. But, since outside of the training set, $f$ and $f'$ are opposite, every correct prediction of $L$ for $f$ outside the training set is a miss-prediction for $f'$ and vice-versa. That is:
$$
\text{Acc}_G(L, f) = 1 - \text{Acc}_G(L, f').
$$
> Hence, if $\text{Acc}_G(L, f) = 0.5 + \delta$, then $\text{Acc}_G(L, f') = 1-0.5-\delta = 0.5 - \delta$.

---

- **Corollary**: for any two learners $L_1$, $L_2$, if $\exists f$ s.t. $\text{Acc}_G(L_1, f) > \text{Acc}_G(L_2, f)$, then, there must $\exists f'$ s.t. $\text{Acc}_G(L_1, f') < \text{Acc}_G(L_2, f')$.

> **Proof**: by contradiction, suppose that $\text{Acc}_G(L_1, f) > \text{Acc}_G(L_2, f)$ $\forall f \in \mathcal{F}$. Then:
$$
\frac{1}{|\mathcal{F}|} \sum_{f \in \mathcal{F}} \text{Acc}_G(L_1, f) > \frac{1}{|\mathcal{F}|} \sum_{f \in \mathcal{F}} \text{Acc}_G(L_2, f),
$$
> but this is absurd since both must be equal to $\frac{1}{2}$.

Fortunately, in the real world, $\mathcal{F}$ is a proper, "_very small_" (_this is a really vague statement without Measure Theory_) subset of $\{0, 1\}^X$: NOT every concept makes sense, hence it is reasonable to study Machine Learning instead of building random guessing classifiers
(:-D).

An important take on from the previous corollary is that we can't expect that our favorite learner is **always** the best. (_Even though, as we just explained, in reality $\mathcal{F}$ is constrained and so there could be some learners which are better than others on average_).

## Bias-Variance decomposition

We're going to investigate a bit more the _under-fitting_ vs _over-fitting_ problem encountered in linear regression and that affects all ML problems.
Consider the usual data generation mechanism: $t = f(\underline{x}) + \epsilon$ with $\mathbb{E}[\epsilon] = 0$ and $\mathbb{V}\text{ar}[\epsilon] = \sigma^2$.
We're going to decompose the expression of the expected (ideal) squared loss as the sum of 3 important quantities. Let $y$ be the hypothesis chosen by our learner after seeing the training set $\mathcal{D}$. Let $t_{\text{test}}$ and $\underline{x}_{\text{test}}$ be generated by the same mechanism which produced $\mathcal{D}$ and that we reminded above. Of course we assume that $t_{\text{test}}$, and $\underline{x}_\text{test}$ are independent from the samples in $\mathcal{D}$ as we assume that all the samples in $\mathcal{D}$ are independent from each other. We're going to take expectation over all the sources of randomness: $\mathcal{D}$, $\underline{x}_\text{test}$, and $\epsilon_{\text{test}}$ (_where $t_{\text{test}} = f(\underline{x}_\text{test}) + \epsilon_\text{test}$_). This provides a good evaluation of how our learning algorithm performs on average.

$$
\mathbb{E}_{\mathcal{D},\underline{x}_\text{test},\epsilon_\text{test}}[(t_\text{test} - y(\underline{x}_\text{test}))^2] = \mathbb{E}_{\underline{x}_\text{test}}[\mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[(t_\text{test} - y(\underline{x}_\text{test}))^2|\underline{x}_\text{test}]] =
$$
$$
= \mathbb{E}_{\underline{x}_\text{test}}[\mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[t_\text{test}^2] - 2 \mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[t_\text{test}y(\underline{x}_\text{test})] + \mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[y^2(\underline{x}_\text{test})]|\underline{x}_\text{test}] =
$$
$$
= \mathbb{E}_{\underline{x}_\text{test}}[\mathbb{V}\text{ar}_{\mathcal{D}, \epsilon_\text{test}}[t_\text{test}] + \mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[t_\text{test}]^2 - 2 \mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[t_\text{test}]\mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[y(\underline{x}_\text{test})] +
$$
$$
+ \mathbb{V}\text{ar}_{\mathcal{D}, \epsilon_\text{test}}[y(\underline{x}_\text{test})] + \mathbb{E}_{\mathcal{D}, \epsilon_\text{test}}[y(\underline{x}_\text{test})]^2|\underline{x}_\text{test}] =
$$
$$
= \mathbb{E}_{\underline{x}_\text{test}}[\sigma^2 + f^2(\underline{x}_\text{test}) - 2f(\underline{x}_\text{test})\mathbb{E}_{\mathcal{D}}[y(\underline{x}_\text{test})] + \mathbb{V}\text{ar}_{\mathcal{D}}[y(\underline{x}_\text{test})] + \mathbb{E}_{\mathcal{D}}[y(\underline{x}_\text{test})]^2|\underline{x}_\text{test}] =
$$
$$
= \mathbb{E}_{\underline{x}_\text{test}}[\sigma^2 + \mathbb{V}\text{ar}_{\mathcal{D}}[y(\underline{x}_\text{test})] + \mathbb{E}_{\mathcal{D}}[f(\underline{x}_\text{test}) - y(\underline{x}_\text{test})]^2|\underline{x}_\text{test}].
$$

---

In the derivation we used the fact that:
- given $\underline{x}_\text{test}$, $y(\underline{x}_\text{test})$ is a function of $\mathcal{D}$ which is independent from $\epsilon_{\text{test}}$, hence, $y(\underline{x}_\text{test})$ and $t_{\text{test}} = f(\underline{x}_\text{test}) + \epsilon_\text{test}$ are independent;
- if the random variable $X$ is independent from $Y$ then:
$$
\mathbb{E}_{Y,Z}[X] = \mathbb{E}_Y[\mathbb{E}_Z[X|Y]] = \mathbb{E}_Y[\mathbb{E}_Z[X]] = \mathbb{E}_Z[X] \mathbb{E}_Y[1] = \mathbb{E}_Z[X];
$$
- $\mathbb{V}\text{ar}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$.

Observe that in the slides $\underline{x}_\text{test}$ is assumed fixed, the only difference is that, under that assumption, we don't need the "external" expectation. Even though this analysis is a bit more general.

We call:
- **irreducible noise** the term $\sigma^2$;
- **bias** the term $\mathbb{E}_{\mathcal{D}}[f(\underline{x}_\text{test}) - y(\underline{x}_\text{test})]$;
- **variance of the model** the term $\mathbb{V}\text{ar}_{\mathcal{D}}[y(\underline{x}_\text{test})]$.

In words we could say that: _expected squared loss = irreducible noise + variance of the model + bias squared_.

- If we sample a training set $\mathcal{D}$ multiple times, we expect to learn a different model $y$. The **bias** is the difference between the truth and the model we expect to learn. It <u>decreases with the size of the hypothesis space</u>, i.e. with the complexity of the models.

- The **variance of the model** is a measure of a how the learned model varies for different datasets. It <u>increases with the size of the hypothesis space</u>. <u>It decreases as the number of samples increases</u>.

This is the reason for the name **bias-variance** tradeoff. It also explains why increasing too much the model complexity worsens the generalization ability of the model.

## Training set and test set

Even if we use the average residual sum of squares as empirical loss, the training error is NOT a good estimate of the expected squared loss. This can be understood even from an empirical point of view:
- from the decomposition we just derived we expect that, starting from a very simple model class and increasing the complexity, the expected squared loss initially decreases since we reduce the _bias squared_ while keeping the variance almost constant since we have enough data; at some point the model complexity becomes too high w.r.t. the available data, the variance starts increasing, and the expected squared loss with it;

---

- conversely, the training loss decreases monotonically with model complexity.

Indeed the behavior of the training loss is more similar to that of the bias term. In particular we say that the training error is an **optimistically biased** estimate of the _prediction error_ (the expected squared loss).

For this reason, to evaluate our model and estimate the prediction error, we need a dataset different from the training set, that we use only for this purpose. This is known as **test set**. Indeed, when we computed the bias-variance decomposition of the generalization error, we introduced the random variables $t_\text{test}$ and $\underline{x}_\text{test}$, independent from $\mathcal{D}$. To have an unbiased estimate of such error we need a dataset independent from $\mathcal{D}$.

## Managing the bias-variance tradeoff

The methods used to manage the bias-variance tradeoff are known as **model selection** techniques (_at the end we will tackle also model ensemble which have the same objective_).
We can distinguish 3 classes of model selection techniques.
- **Feature selection**: allows to identify a _subset of input features_ that are most related to the output;
- **Regularization**: all the input features are used, but estimated coefficients are **shrunken towards zero**, thus reducing the variance;
- **Dimension reduction**: the input variables are **projected** into a lower-dimensional subspace.

Observe that reducing the number of features can be beneficial for solving a learning problem. This is related to the so-called **curse of dimensionality**. It is related to the **exponential increase in volume** associated with adding **extra dimensions** to the input space. Working with high-dimensional data is **difficult** both from the computational point of view and from the statistical point of view. To prevent the increase in model variance we need many samples, which scale as $N^d$ where $d$ is the number of dimensions of the features space.
A common pitfall is that if we can't solve a problem with a few features, adding **more features** seems like a good idea. However, since the number of samples usually **stays the same**, the method with more features is likely to **perform worse**.

### Feature selection

#### Best subset selection

The **best subset selection** technique is a feature selection technique which works as follows.
1. Let $\mathcal{M}_0$ denote the **null model**, which contains no input feature: it just predicts the sample mean for each observation (_which is constant and does not depend on any of the features_);

---

2. For $k \in \{ 1, \ldots, M \}$:
> 2.1 Fit all $\binom{M}{k}$ models that contain exactly $k$ features;
> 2.2 Pick the best among these $\binom{M}{k}$ models and call it $\mathcal{M}_k$. Here **best** is defined as having the smallest RSS, or equivalently the largest $R^2$.
3. Select a **single best model** from among $\mathcal{M}_0, \ldots, \mathcal{M}_M$ using some criterion (cross-validation, AIC, BIC, ... [_see later_]).

Best subset selection has some problem when $M$ is **large**: it is subject to over-fitting and has an high computational cost.
To solve these problems we can adopt 3 **meta-heuristics**:
- **Filter**: we rank the features through a criterion independent from the chosen model, like the correlation with the target, and keep only the best ones;
- **Embedded**: the learning algorithm exploits its own variable selection technique (_this happens for example in LASSO which usually learns sparse linear models_);
- **Wrapper**: we evaluate some of the subsets of features according to some criterion. The possible criteria are:
> - **Forward selection**: we start from an **empty model** and **add** features **one-at-a-time** (_every time we keep the best_);
> - **Backward elimination**; we start with **all the features** and **remove** the least useful feature, **one-at-a-time**.

### Choosing the optimal model

THe model containing **all the features** will always have the **smallest training error**. We wish to choose a model with **low test error**, not a low training error.
THerefore, $\text{RSS}$ and $R^2$ are **not** suitable for selecting the best model among a collection of models with different numbers of features.
There are two approaches to **estimate the test error**:
- we can do a **direct estimation** using a **validation** approach;
- or we can make an **adjustment** to the training error to account for **model complexity**.

#### The validation set

As explained above, we want to find a way of estimating the test error to choose the optimal model. We can't use the training error since it is not a good estimate of the test error and the most complex model will always have the smallest training error.
We can't use the test set either. Indeed, if we were to choose the model according to the test error, the model would become _dependent_ on the test set and the test error would stop being an unbiased estimate of the generalization error. In simpler words, if we learn from the test set, the test set becomes part of the training set and the test error becomes like the training error.

---

Hence we need a third set of data, known as **validation set**, which we can use to choose the best model family, but that we can't use to evaluate the performance of the learned model (for that we use the test set, otherwise the estimate would be biased) or to learn its parameters (for that we use the training set).

Observe that simply using the _validation error_ for model selection could lead to overfitting on the validation set, especially if it doesn't contain many samples.
We need some more sophisticated approaches.

##### Leave-One-Out cross validation

**Leave-Ont-Out** (**LOO**) cross validation works as follows.
Let $\mathcal{D}$ be our dataset composed of $N$ samples. Let $n \in \{ 1, \ldots, N \}$.  Let $\mathcal{D} \setminus \{ n \}$ be the training set, without the $n$-th sample. Finally, let $y_{\mathcal{D} \setminus \{ n \}}$ be the model learned by our algorithm on the training set $\mathcal{D} \setminus \{ n \}$.
We can estimate the generalization error of $y_{\mathcal{D} \setminus \{ n \}}$ on the single remaining point: $(t_n - y_{\mathcal{D} \setminus \{ n \}}(\underline{x}_n))^2$. Observe that the estimate is almost unbiased: actually it is a bit pessimistically biased since to learn the actual model we will use the whole dataset $\mathcal{D}$ instead of $\mathcal{D} \setminus \{ n \}$. But it has a lot of variance: we are using only one sample to estimate the error. We can solve the problem by taking the average over all $n$:
$$
L_{\text{LOO}} = \frac{1}{N} \sum_{n=1}^N (t_n - y_{\mathcal{D} \setminus \{ n \}}(\underline{x}_n))^2.
$$

This approach is great from the statistical point of view since it provides an estimate with small variance and small bias, but it is problematic from the computational point of view since we have to retrain the model $N$ times to have a single estimate.

##### $k$-fold cross validation

To overcome the computational problems of LOO cross validation we can use $k$-fold cross validation.
In $k$-fold cross validation we **randomly** divide the training data into $k$ equal parts $\mathcal{D}_1, \ldots, \mathcal{D}_k$. For each $i \in \{ 1, \ldots, k \}$ we learn the model $y_{\mathcal{D}\setminus\mathcal{D}_i}$ using as training set $\mathcal{D} \setminus \mathcal{D}_i$. Then we estimate the error of $y_{\mathcal{D}\setminus\mathcal{D}_i}$ on the validation set $\mathcal{D}_i$:
$$
L_{\mathcal{D}_i} = \frac{k}{N} \sum_{(\underline{x}_n, t_n) \in \mathcal{D}_i} (t_n - y_{\mathcal{D} \setminus \mathcal{D}_i}(\underline{x}_n))^2.
$$
Finally, the $k$-fold cross validation error is the average of the previous:
$$
L_{k\text{-fold}} = \frac{1}{k} \sum_{i=1}^k L_{D_i}.
$$
For the reason we discussed earlier, $k$-fold cross validation is more pessimistically biased than LOO cross validation, but it is $\frac{N}{k}$ times faster from the computational point of view.

---

Indeed, LOO cross validation is $N$-fold cross validation.

#### Adjustment techniques

**Adjustment techniques** consist in modifying the loss function to account for model complexity. We will list some possibilities.

- $C_p = \frac{1}{N}(\text{RSS} + 2d\hat{\sigma}^2)$ where $d$ is the total number of parameters, $\hat{\sigma}^2$ is an estimate of the variance of the noise $\epsilon$;
- $\text{AIC} = -2\log L + 2d$ where $L$ is the maximized value of the likelihood function for the estimated model;
- $\text{BIC} = \frac{1}{N}(\text{RSS} + \log(N)d\hat{\sigma}^2)$: it replaces the $2d\hat{\sigma}^2$ in $C_p$ with $\log(N)d\hat{\sigma}^2$, hence it selects smaller models;
- $\text{Adjusted}R^2 = 1- \frac{\text{RSS}/(N-d-1)}{\text{TSS}/(N-1)}$: <u>differently from the other criteria, here a **large value** indicates a model with a **small test error**</u>.

### Regularization a.k.a. Shrinkage Methods

We have already seen regularization approaches applied to **linear models** (_ridge regression_, _LASSO_). Such methods **shrink** the parameters towards **zero**.
It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking coefficient estimates can significantly reduce the variance.

As for feature selection, for ridge regression and LASSO we require a method to determine which of the model under consideration is **best**. So, we need a method for selecting the tuning parameter $\lambda$. **Cross validation** provides a simple way to tackle this problem. We choose a grid of $\lambda$ values, and compute the cross-validation error rate for each value of $\lambda$. We then select the tuning parameter value for which the cross-validation error is **smallest**.
Finally, the model is **re-fit** using **all** of the available observations and the selected value of the tuning parameter.

### Dimension reduction

The previous approaches operate on the **original features**. Dimension reduction methods **transform** the original features and then the model is learned on the **transformed variables**. Dimension reduction is an **unsupervised learning** approach.
There are many techniques to perform dimensionality reduction: we will tackle **Principal Component Analysis** (**PCA**).

#### Principal Component Analysis (PCA)

The idea behind **PCA** is to project the input dataset onto the subspace which accounts for most of the variance. Let's formalize the algorithm.
Let $\underline{x}_1, \ldots, \underline{x}_N \in \mathbb{R}^M$ (where $M$ is the number of features) be the input samples in our dataset.

---

Let
$$
\overline{\underline{x}} = \frac{1}{N} \sum_{n=1}^N \underline{x}_n \text{ be the sample mean of the input samples.}
$$
The sample covariance matrix is:
$$
S = \frac{1}{N-1} \sum_{n=1}^N (\underline{x}_n - \overline{\underline{x}}_n) (\underline{x}_n - \overline{\underline{x}}_n)^T.
$$

1. **Initialization step**: we want to find a direction $\underline{u}_1 \in \mathbb{R}^M$, $||\underline{u}_1||_2 = 1$ s.t. the variance of the transformed feature $\phi_1 = \underline{u}_1^T \underline{x}$ is maximum. Observe that:
$$
\overline{\phi}_1 = \frac{1}{N} \sum_{n=1}^N \phi_{1,n} = \frac{1}{N} \sum_{n=1}^N \underline{u}_1^T \underline{x}_n = \underline{u}_1^T \underline{\overline{x}}.
$$
> And:
$$
\hat{\mathbb{V}\text{ar}}[\phi_1] = \frac{1}{N-1} \sum_{n=1}^N (\phi_{1,n} - \overline{\phi}_1)^2 = \frac{1}{N-1} \sum_{n=1}^N \underline{u}_1^T (\underline{x}_n - \underline{\overline{x}}) (\underline{x}_n - \underline{\overline{x}})^T \underline{u}_1 = \underline{u}_1^T S \underline{u}_1.
$$
> Hence, we need to solve the following optimization problem:
$$
\max \underline{u}_1^T S \underline{u}_1
$$
$$
\text{s.t.}
$$
$$
\underline{u}_1^T \underline{u}_1 = 1.
$$
> We can exploit Lagrange's multipliers theorem (_the constraint gradient is unique and there is at least one optimum with gradient different from $\underline{0}_M$, hence the set of constraint gradients is linearly independent_) to derive a necessary condition that the optimal solutions to the above problem must satisfy:
$$
\begin{cases}
2 S \underline{u}_1 = \lambda_1 2 \underline{u}_1 \\
\underline{u}_1^T \underline{u}_1 =1 
\end{cases} \text{ iff }
\begin{cases}
S \underline{u}_1 = \lambda_1 \underline{u}_1 \\
\underline{u}_1^T \underline{u}_1 = 1
\end{cases}.
$$
> Hence the solution to the above optimization problem is an eigenvector of $S$ with magnitude $1$. Hence the maximum is attained by the eigenvector $\underline{u}_1$, with magnitude $1$, associated to the largest eigenvalue $\lambda_1$.
Furthermore, observe that, for such optimal solution:
$$
\hat{\mathbb{V}\text{ar}}[\phi_1] = \underline{u}_1 S \underline{u}_1  = \lambda_1 \underline{u}_1^T \underline{u}_1 = \lambda_1.
$$

2. **Iteration**: we want to generalize the result for the $k$ orthogonal directions which produce features with the largest sum of variances.

----

> Suppose that $\underline{u}_1, \ldots, \underline{u}_{k-1}$ have already been found and correspond to $k-1$ normalized eigenvectors of $S$ associated with the $k-1$ largest eigenvalues.
We want to solve:
$$
\max \underline{u}_k^T S \underline{u}_k
$$
$$
\text{s.t.}
$$
$$
\underline{u}_k^T \underline{u}_i = \delta_{ki} \text{ for } i \in \{ 1, \ldots, k \}.
$$
> Let's use again Lagrange's multipliers:
$$
\frac{\partial}{\partial \underline{u}_k}[\underline{u}_k^T \underline{u}_1 - \delta_{ki}] = \begin{cases}
\underline{u}_i^T \text{ if } i \neq k \\
2 \underline{u}_k^T \text{ if } i = k
\end{cases}.
$$
> Then:
$$
\frac{\partial}{\partial \underline{u}_k} \begin{bmatrix} \underline{u}_k^T \underline{u}_1 \\ \vdots \\ \underline{u}_k^T \underline{u}_{k-1} \\ \underline{u}_k^T \underline{u}_{k-1} \end{bmatrix} = \begin{bmatrix} \underline{u}_1^T \\ \vdots \\ \underline{u}_{k-1}^T \\ 2 \underline{u}_k^T \end{bmatrix} \text{, hence the necessary conditions are:}
$$
$$
\begin{cases}
2 S \underline{u}_k = \begin{bmatrix} \underline{u}_1 & \ldots & \underline{u}_{k-1} & 2 \underline{u}_k \end{bmatrix} \begin{bmatrix} \alpha_1 \\ \vdots \\ \alpha_{k-1} \\ \lambda_{k-1} \end{bmatrix} \\
\underline{u}_k^T \underline{u}_i = \delta_{ki} \text{ for } i \in \{ 1, \ldots, k \}
\end{cases} \text{ iff }
$$
$$
\begin{cases}
2 S \underline{u}_k = \alpha_1 \underline{u}_1 + \ldots + \alpha_{k-1} \underline{u}_{k-1} + 2 \lambda_k \underline{u}_k \\
\underline{u}_k^T \underline{u}_i = \delta_{ki} \text{ for } i \in \{ 1, \ldots, k \}
\end{cases}.
$$
> Observe that, if $i \in \{ 1, \ldots, k-1 \}$, then:
$$
\underline{u}_i^T 2 S \underline{u}_k = 2 \lambda_i \underline{u}_i^T \underline{u}_k = 2 \lambda_i \delta_{ki};
$$
> while:
$$
\underline{u}_i^T (\alpha_1 \underline{u}_1 + \ldots + \alpha_{k-1} \underline{u}_{k-1} + \lambda_k \underline{u}_k) = \begin{cases} \alpha_i \text{ if } i \in \{ 1, \ldots, k-1 \} \\ 2 \lambda_k \text{ if } i = k \end{cases}.
$$
> Hence, if $i \in \{ 1, \ldots, k-1 \}$:
$$
0 = 2\lambda_i \delta_{ik} = \alpha_i.
$$
> Then it must be:
$$
S \underline{u}_k = \lambda_k \underline{u}_k.
$$

---

> We proved that $\underline{u}_k$ is an eigenvector of $S$, in particular it is the one with largest eigenvalue $\lambda_k$ among those orthogonal to the ones that we've already chosen.

The dataset with new features is given by:
$$
X' = \begin{bmatrix}
\underline{x}_1^T \underline{u}_1 & \ldots & \underline{x}_1^T \underline{u}_n \\
\vdots & \vdots & \vdots \\
\underline{x}_n^T \underline{u}_1 & \ldots & \underline{x}_n^T \underline{u}_n \\
\end{bmatrix} = \begin{bmatrix} \underline{x}_1^T \\ \vdots \\ \underline{x}_n^T \end{bmatrix} \begin{bmatrix} \underline{u}_1 & \ldots & \underline{u}_k \end{bmatrix} = X E_k
$$
where $E_k = \begin{bmatrix} \underline{u}_1 & \ldots & \underline{u}_k \end{bmatrix}$.
We define:
$$
\frac{\lambda_k}{\sum_{i=1}^M \lambda_i}
$$
as the **fraction of variance** captured by the $k$-th eigenvector.

Transforming the reduced dimensionality projection back into the original space gives a reduced dimensionality **reconstruction** of the original data (in particular it is the one with the smallest reconstruction error in virtue of the Eckart-Young theorem, see NAML summaries).

PCA can help reduce the **computational load** on the learning technique. Furthermore, reducing the dimension implies a simpler hypothesis space, with less risk of overfitting.
PCA can also be seen as **noise reduction**.
The are some **caveats**:
- it fails when data consists of **multiple clusters**;
- the direction with the greatest variance may not be the most informative for out learning task;
- when $M$ is large it can become computationally heavy (the SVD can help);
- PCA computes **linear** combinations of features, but data often lies on a **nonlinear** manifold (see the ISOMAP method).

### Model ensemble

The methods seen so far can reduce bias by increasing variance or _vice versa_.
Is it possible to reduce the variance **without increasing the bias**?
Yes, with **Bagging**!
Is it possible to reduce also the bias **without increasing the variance**?
yes, with **Boosting**!

Bagging and Boosting are **meta-algorithms**, they can be applied to other learning algorithms. The basic idea behind both is to learn several models and combine them instead of learning just one model.
Typically this improves the accuracy by a lot.